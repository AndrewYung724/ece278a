{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJmCCVjpadTy"
      },
      "source": [
        "# ECE 3 - Lab 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elx4T-wCahcc"
      },
      "source": [
        "# Least Squares Introduction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd0XNMs4SXRU"
      },
      "source": [
        "$\\color{#EF5645}{\\text{Definition}}$: Let be given a $m \\times n$ matrix $A$ and $m$-vector $b$. The least squares problem is the problem of choosing an $n$-vector $x$ to minimize:\n",
        "$$‖Ax − b‖^2.$$\n",
        "\n",
        "- If $\\hat x$ is a solution of the linear equation $Ax = b$, then $\\hat x$ is a solution of the least square problem. The converse is not true. (Explain Why)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p66i3xe9HNHu"
      },
      "source": [
        "## Exercise 1\n",
        "Show $$\\hat x = (A^T A)^{-1} A^Tb = A^\\dagger b$$ is the unique solution to the least square problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQdMPmXeVdpH"
      },
      "source": [
        "## Exercise 2\n",
        "If $A$ has $QR$ factorization $A = QR$, show that $A\\hat{x} = QQ^Tb$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UERrqwIM26e"
      },
      "source": [
        "# Least Square Data Fitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfuZ9FTTNOo7"
      },
      "source": [
        "Given\n",
        "$$x^{(1)}, . . . , x^{(N)}, y^{(1)}, . . . , y^{(N)}$$\n",
        "\n",
        "We try to find the function $\\hat{f}$ that maps $x$ to $y$.\n",
        "$$\\hat{f}(x) = \\theta_1 f_1(x) + ... + \\theta_p f_p(x)$$\n",
        "\n",
        "To measure how good the $\\hat{f}$ we found is, we use $\\bf{residual}/error$. The best $\\hat{f}$ is the one giving the smallest error.\n",
        "$$r_i = y^{(i)} - \\hat{y}^{(i)}.$$\n",
        "\n",
        "The problem is called \"Least Square\" because we use the squared of the residual rather than the residual itself so that we do not worry about negative versus positive residual.\n",
        "\n",
        "Formulated as a least square problem:\n",
        "\n",
        "Define the $N \\times p$ matrix $A$ with elements $A_{ij} = f_j(x^{(i)})$, such that $\\hat y =A \\theta$. The least square data fitting problem amounts to choose $\\theta$ that minimizes:\n",
        "$$||A\\theta - y||^2$$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3x6bnjWQIyc"
      },
      "source": [
        "## Exercise 3\n",
        "Given data points $(-0.5,6), (0,3), (1.1,0), (1.6,0),(2.5,3.2)$ and $\\hat{f} = \\theta_1x^2 + \\theta_2x + \\theta_3$. Use Python to find the optimal $\\theta$."
      ]
    }
  ]
}